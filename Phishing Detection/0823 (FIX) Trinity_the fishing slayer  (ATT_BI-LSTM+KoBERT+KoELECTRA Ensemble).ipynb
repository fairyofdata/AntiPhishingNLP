{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyNG3KYE42zbkWvsBRpUYYWg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# -------------------------경로 재설정 및 필수 모듈 재설치----------------------------\n","# 경로 지정 (필요 시 드라이브 마운트 제외)\n","from google.colab import drive\n","drive.mount('/content/drive')\n","#bi_lstm_attention_tokenizer.pickle\n","TOKENPATH_ABL = '/content/drive/MyDrive/finalvoice/bi_lstm_attention_tokenizer.pickle'\n","#bi_lstm_attention_model.h5\n","MODELPATH_ABL = \"/content/drive/MyDrive/finalvoice/bi_lstm_attention_model.h5\"\n","#/kobert_model\n","model_path_KBT = \"/content/drive/MyDrive/finalvoice/kobert_model\"\n","#/koelectra_saved_model\n","model_path_KER = '/content/drive/MyDrive/finalvoice/koelectra_saved_model'\n","# 라이브러리 설치 (필수)\n","!pip install konlpy\n","!pip install accelerate>=0.20.1 transformers[torch]\n","'''\n","!pip install transformers\n","!pip install accelerate>=0.20.1\n","!pip install transformers[torch]\n","!pip install accelerate transformers[torch]\n","'''\n","# ---------------------------------Attention+Bi-LSTM Model Setting---------------------------------\n","import pandas as pd\n","# 커스텀레이어 정의\n","from tensorflow.keras.layers import Layer\n","class AttentionLayer(Layer):\n","    def __init__(self, **kwargs):\n","        super(AttentionLayer, self).__init__(**kwargs)\n","    def build(self, input_shape):\n","        # Attention 가중치를 위한 weight 생성\n","        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1), initializer=\"normal\")\n","        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1), initializer=\"zeros\")\n","        super(AttentionLayer, self).build(input_shape)\n","    def call(self, x):\n","        # Attention score 계산\n","        e = tf.keras.backend.tanh(tf.keras.backend.dot(x, self.W) + self.b)\n","        a = tf.keras.backend.softmax(e, axis=1)\n","        output = x * a\n","        return tf.keras.backend.sum(output, axis=1)\n","    def compute_output_shape(self, input_shape):\n","        return (input_shape[0], input_shape[-1])\n","# 필요 모듈, tokenizer, 모델 불러오기\n","from konlpy.tag import Okt\n","okt = Okt()\n","import pickle\n","import os\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import load_model\n","abl_model = load_model(MODELPATH_ABL, custom_objects={'AttentionLayer': AttentionLayer})\n","with open(TOKENPATH_ABL, 'rb') as handle:\n","    keras_tokenizer = pickle.load(handle)\n","# 확률연산\n","def predict_label_prob_ABL(sentence):\n","    # 문장 토큰화\n","    tokenized_sentence = okt.morphs(sentence)\n","    # 토큰화된 문장을 숫자 시퀀스로 변환 (Keras의 Tokenizer 사용)\n","    sequence = keras_tokenizer.texts_to_sequences([tokenized_sentence])\n","    # 패딩 처리\n","    MAX_LENGTH = 100\n","    padded_sequence = pad_sequences(sequence, maxlen=MAX_LENGTH)\n","    # 예측 수행\n","    prediction_ABL = abl_model.predict(padded_sequence)\n","    # 예측 확률 반환\n","    return prediction_ABL[0][0]\n","'''\n","user_input = input(\"문장을 입력하세요: \")\n","probs_ABL = predict_label_prob_ABL(user_input)\n","print(f\"라벨 0의 확률: {1 - probs_ABL:.4f}\")\n","print(f\"라벨 1의 확률: {probs_ABL:.4f}\")\n","'''\n","# ---------------------------------KoBERT Model Setting---------------------------------\n","import torch\n","from transformers import BertTokenizer, BertForSequenceClassification\n","import torch.nn.functional as F\n","# KoBERT 토크나이저 불러오기\n","tokenizer_KBT = BertTokenizer.from_pretrained('monologg/kobert')\n","# 모델 구조 불러오기\n","model_KBT = BertForSequenceClassification.from_pretrained('monologg/kobert')\n","# 저장된 가중치 불러오기\n","model_KBT.load_state_dict(torch.load(model_path_KBT, map_location=torch.device('cpu')))\n","model_KBT.eval()\n","def get_prediction_probabilities_KBT(text):\n","    # 텍스트를 토크나이징\n","    inputs = tokenizer_KBT.encode_plus(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n","    # 예측\n","    with torch.no_grad():\n","        outputs = model_KBT(**inputs)\n","        logits = outputs.logits\n","    # 확률을 계산하기 위해 softmax 함수 사용\n","    probs_KBT = F.softmax(logits, dim=-1)\n","    # 확률값 반환\n","    return probs_KBT[0].tolist()\n","'''\n","# 사용자 입력 받기\n","user_input = input(\"텍스트를 입력하세요: \")\n","probs_KBT = get_prediction_probabilities_KBT(user_input)\n","# 각 라벨의 확률 출력\n","print(f\"라벨 0의 확률: {probs_KBT[0]:.4f}\")\n","print(f\"라벨 1의 확률: {probs_KBT[1]:.4f}\")\n","'''\n","# ---------------------------------KoELECRTA Model Setting---------------------------------\n","from transformers import ElectraForSequenceClassification, ElectraTokenizer\n","import torch.nn.functional as F\n","# 1. 저장된 모델과 토크나이저 불러오기\n","tokenizer_KER = ElectraTokenizer.from_pretrained(model_path_KER)\n","model = ElectraForSequenceClassification.from_pretrained(model_path_KER)\n","def get_prediction_probabilities_KER(user_input):\n","    # 텍스트를 토크나이징\n","    inputs = tokenizer_KER(user_input, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n","    # 모델을 평가 모드로 설정\n","    model.eval()\n","    # 예측\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","        logits = outputs.logits\n","    # 확률을 계산하기 위해 softmax 함수 사용\n","    probs_KER = F.softmax(logits, dim=-1)\n","    return probs_KER[0].tolist()\n","'''\n","# 사용자 입력 받기\n","user_input = input(\"텍스트를 입력하세요: \")\n","probs_KER = get_prediction_probabilities_KER(user_input)\n","# 각 라벨의 확률 출력\n","print(f\"라벨 0의 확률: {probs_KER[0]:.4f}\")\n","print(f\"라벨 1의 확률: {probs_KER[1]:.4f}\")\n","'''\n","# -----------------------------Trinity(Ensemble) Test------------------------------\n","while True:\n","  user_input = input(\"문장을 입력하세요 (종료하려면 공백 입력): \")\n","  if not user_input.strip():\n","    break\n","  # 각 모델에서 예측 확률을 가져옵니다.\n","  probs_ABL = predict_label_prob_ABL(user_input)\n","  probs_KBT = get_prediction_probabilities_KBT(user_input)\n","  probs_KER = get_prediction_probabilities_KER(user_input)\n","  # 각 모델에서 예측한 확률을 저장\n","  bi_lstm_attention_prob_1 = probs_ABL\n","  bi_lstm_attention_prob_0 = 1 - probs_ABL\n","  kobert_prob_1 = probs_KBT[1]\n","  kobert_prob_0 = probs_KBT[0]\n","  koelectra_prob_1 = probs_KER[1]\n","  koelectra_prob_0 = probs_KER[0]\n","  Trinity_1 = (bi_lstm_attention_prob_1 + kobert_prob_1 + koelectra_prob_1) / 3\n","  Trinity_0 = (bi_lstm_attention_prob_0 + kobert_prob_0 + koelectra_prob_0) / 3\n","  #앙상블 최종 결과만 볼 경우\n","  print(f\"보이스피싱 가능성: {Trinity_1:.2f}\",\"\\n\")\n","  #데이터프레임으로 출력할 경우\n","'''\n","  df = pd.DataFrame({\n","      'Model': ['Attention+Bi-LSTM', 'KoBERT', 'KoELECRTA', 'Trinity(Ensemble)'],\n","      'Fish': [bi_lstm_attention_prob_1, kobert_prob_1, koelectra_prob_1, Trinity_1],\n","      'Clear': [bi_lstm_attention_prob_0, kobert_prob_0, koelectra_prob_0, Trinity_0]\n","  })\n","  df = df.round(2)\n","  print(df)\n","'''"],"metadata":{"id":"jFGfpjoqrHb7"},"execution_count":null,"outputs":[]}]}