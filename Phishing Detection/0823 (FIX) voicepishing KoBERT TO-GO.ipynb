{"cells":[{"cell_type":"markdown","source":["##KoBERT TO-GO"],"metadata":{"id":"Apj3r_VOFxqd"}},{"cell_type":"code","source":["!pip install transformers\n","!pip install gradio"],"metadata":{"id":"uSzCDGhUV1rJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from transformers import BertTokenizer, BertForSequenceClassification\n","import torch.nn.functional as F\n","\n","# 저장된 모델 경로\n","model_path = \"/content/drive/MyDrive/finalvoice/kobert_model\"\n","\n","# KoBERT 토크나이저 불러오기\n","tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n","\n","# 모델 구조 불러오기\n","model = BertForSequenceClassification.from_pretrained('monologg/kobert')\n","# 저장된 가중치 불러오기\n","model.load_state_dict(torch.load(model_path))\n","model.eval()\n","\n","def get_prediction_probabilities(text):\n","    # 텍스트를 토크나이징\n","    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n","\n","    # 예측\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","        logits = outputs.logits\n","\n","    # 확률을 계산하기 위해 softmax 함수 사용\n","    probs = F.softmax(logits, dim=-1)\n","\n","    # 결과를 딕셔너리 형태로 반환\n","    results = {}\n","    for i, prob in enumerate(probs[0]):\n","        results[f\"라벨 {i}\"] = float(prob)\n","\n","    return results\n","\n","# 사용자 입력 받기\n","user_input = input(\"텍스트를 입력하세요: \")\n","prediction_results = get_prediction_probabilities(user_input)\n","print(prediction_results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pj4rQ-VdoOh9","executionInfo":{"status":"ok","timestamp":1692730990918,"user_tz":-540,"elapsed":9541,"user":{"displayName":"hufs_1jo","userId":"13927265952478603851"}},"outputId":"04b692a9-8d33-4d9c-cb37-cb2e60c55554"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["텍스트를 입력하세요: 텍스트를 입력하세요.\n","{'라벨 0': 0.005742729641497135, '라벨 1': 0.9942572116851807}\n"]}]},{"cell_type":"code","source":["# KoBERT Gradio 구현\n","'''\n","import torch\n","from transformers import BertTokenizer, BertForSequenceClassification\n","import torch.nn.functional as F\n","import gradio as gr\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# 저장된 모델 경로\n","model_path = \"/content/drive/MyDrive/finalvoice/kobert_model\"\n","\n","# KoBERT 토크나이저 불러오기\n","tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n","\n","# 모델 구조 불러오기\n","model = BertForSequenceClassification.from_pretrained('monologg/kobert')\n","# 저장된 가중치 불러오기\n","model.load_state_dict(torch.load(model_path))\n","model.eval()\n","\n","def get_prediction_probabilities(text):\n","    # 텍스트를 토크나이징\n","    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n","\n","    # 예측\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","        logits = outputs.logits\n","\n","    # 확률을 계산하기 위해 softmax 함수 사용\n","    probs = F.softmax(logits, dim=-1)\n","\n","    # 결과를 딕셔너리 형태로 반환\n","    results = {}\n","    for i, prob in enumerate(probs[0]):\n","        results[f\"라벨 {i}\"] = float(prob)\n","\n","    return results\n","\n","# gr.Interface를 사용하여 웹 인터페이스 구성\n","interface = gr.Interface(fn=get_prediction_probabilities,\n","                         inputs=\"text\",\n","                         outputs=\"label\",\n","                         live=True,\n","                         title=\"KoBERT 라벨 예측\",\n","                         description=\"입력된 텍스트의 라벨 예측 확률을 출력합니다.\")\n","interface.launch()\n","'''"],"metadata":{"id":"63YNZPV4nRRk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# KoBERT 소스 코드\n","'''\n","# 필요한 라이브러리\n","import torch\n","from torch import nn\n","from transformers import BertTokenizer, BertForSequenceClassification\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AdamW\n","from sklearn.metrics import accuracy_score\n","import pandas as pd\n","# df_93이라는 데이터가 있다고 가정\n","df_93 = pd.read_csv(\"/content/drive/MyDrive/KorCCViD_v1.3_fullcleansed.csv\")\n","# 데이터를 학습 데이터와 테스트 데이터로 나눕니다.\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(df_93['Transcript'], df_93['Label'], test_size=0.2, random_state=42)\n","# KoBERT 토크나이저 로드\n","tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n","# 데이터 토큰화\n","class KoBERTDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_len):\n","        self.texts = list(texts)\n","        self.labels = list(labels)\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, index):\n","        text = str(self.texts[index])\n","        label = self.labels[index]\n","        inputs = self.tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            pad_to_max_length=True,\n","            return_attention_mask=True,\n","            truncation=True\n","        )\n","\n","        return {\n","            'input_ids': torch.tensor(inputs['input_ids'], dtype=torch.long),\n","            'attention_mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),\n","            'labels': torch.tensor(label, dtype=torch.long)\n","        }\n","\n","# DataLoader 설정\n","BATCH_SIZE = 16\n","MAX_LEN = 128\n","train_data = KoBERTDataset(X_train.reset_index(drop=True), y_train.reset_index(drop=True), tokenizer, MAX_LEN)\n","train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n","test_data = KoBERTDataset(X_test.reset_index(drop=True), y_test.reset_index(drop=True), tokenizer, MAX_LEN)\n","test_loader = DataLoader(test_data, batch_size=BATCH_SIZE)\n","\n","\n","# 모델 정의\n","model = BertForSequenceClassification.from_pretrained('monologg/kobert', num_labels=2)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","# 학습 설정\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","epochs = 10\n","from tqdm.notebook import tqdm\n","\n","# 학습 진행\n","for epoch in range(epochs):\n","    model.train()\n","    total_loss = 0\n","\n","    # tqdm을 train_loader에 적용\n","    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\", leave=False)\n","    for batch_idx, batch in enumerate(progress_bar):\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs[0]\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","        # 진행 상황 업데이트\n","        progress_bar.set_postfix({'loss': total_loss / (batch_idx + 1)})\n","\n","    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss/len(train_loader)}\")\n","import torch\n","\n","# 기존의 테스트 코드\n","model.eval()\n","all_preds = []\n","all_labels = []\n","with torch.no_grad():\n","    for batch in test_loader:\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","        outputs = model(input_ids, attention_mask=attention_mask)\n","        _, preds = torch.max(outputs[0], 1)\n","        all_preds.extend(preds.cpu().numpy())\n","        all_labels.extend(labels.cpu().numpy())\n","\n","# 정확도 평가\n","accuracy = accuracy_score(all_labels, all_preds)\n","print(f\"Accuracy: {accuracy * 100:.2f}%\")\n","# 모델 저장하기\n","save_path = \"/content/drive/MyDrive/finalvoice/kobert_model\"\n","torch.save(model.state_dict(), save_path)\n","print(f\"모델이 {save_path}에 저장되었습니다.\")\n","'''"],"metadata":{"id":"jhVUsYdlVZGL"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"12wODlwrzhPcB1uDc1rLVSVqBhwwS73gM","timestamp":1692730649323},{"file_id":"1pdI9cIgk2IXsWvrxvaDwGLdvfR7BxXXV","timestamp":1692691452614},{"file_id":"1cY7fqqzUghF1e4k4duNKuPKLg557SI45","timestamp":1692679428059}],"machine_shape":"hm","gpuType":"T4","mount_file_id":"1pdI9cIgk2IXsWvrxvaDwGLdvfR7BxXXV","authorship_tag":"ABX9TyOot882z6gUmahMAjnG2RE9"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}