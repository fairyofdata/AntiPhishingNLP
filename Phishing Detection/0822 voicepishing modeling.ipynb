{"cells":[{"cell_type":"markdown","source":["###1. 학습 준비"],"metadata":{"id":"Apj3r_VOFxqd"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ah3tzjOMLlK2"},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"]},{"cell_type":"code","source":[],"metadata":{"id":"4ffmd0WZ7vNL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692691287589,"user_tz":-540,"elapsed":14963,"user":{"displayName":"hufs_1jo","userId":"13927265952478603851"}},"outputId":"1d79882c-d8a9-4698-8ea8-1b11ce070f22"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ND-nuPx_MU9e"},"outputs":[],"source":["# df_93이라는 데이터가 있다고 가정\n","df_93 = pd.read_csv(\"/content/drive/MyDrive/KorCCViD_v1.3_fullcleansed.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QOHg6i0xLiKf"},"outputs":[],"source":["# 데이터를 학습 데이터와 테스트 데이터로 나눕니다.\n","X_train, X_test, y_train, y_test = train_test_split(df_93['Transcript'], df_93['Label'], test_size=0.2, random_state=42)"]},{"cell_type":"code","source":["!pip install konlpy"],"metadata":{"id":"53xSkbgPQ-Dr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692691294144,"user_tz":-540,"elapsed":6177,"user":{"displayName":"hufs_1jo","userId":"13927265952478603851"}},"outputId":"d67d9934-d384-4324-bec0-9071ddd49645"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting konlpy\n","  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n","  Downloading JPype1-1.4.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.23.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.1)\n","Installing collected packages: JPype1, konlpy\n","Successfully installed JPype1-1.4.1 konlpy-0.6.0\n"]}]},{"cell_type":"code","source":["import os\n","from konlpy.tag import Okt\n","\n","# Okt 객체 생성\n","okt = Okt()\n","\n","def tokenize(texts, saved_filename):\n","    \"\"\"주어진 텍스트 리스트를 토큰화하고 진행 상황을 출력합니다.\"\"\"\n","    tokenized_texts = []\n","\n","    # 이전에 저장된 토큰화 결과가 있다면 불러옵니다.\n","    if os.path.exists(saved_filename):\n","        with open(saved_filename, 'r', encoding='utf-8') as f:\n","            tokenized_texts = [line.strip() for line in f]\n","\n","    start_idx = len(tokenized_texts)\n","    for idx, text in enumerate(texts[start_idx:]):\n","        if (idx + 1) % 200 == 0:\n","            print(f\"{idx + 1 + start_idx}개의 텍스트를 토큰화했습니다.\")\n","            # 중간 결과를 저장합니다.\n","            with open(saved_filename, 'a', encoding='utf-8') as f:\n","                for tokens in tokenized_texts[idx-199:idx+1]:\n","                    f.write(' '.join(tokens) + '\\n')\n","\n","        tokenized_texts.append(okt.morphs(text))\n","\n","    return tokenized_texts\n","\n","# 학습 데이터와 테스트 데이터를 토큰화합니다.\n","X_train_tokenized = tokenize(X_train, 'X_train_tokenized.txt')\n","X_test_tokenized = tokenize(X_test, 'X_test_tokenized.txt')\n","\n","print(\"토큰화 완료!\")\n"],"metadata":{"id":"AfT7RlFDFHdE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692691325201,"user_tz":-540,"elapsed":24510,"user":{"displayName":"hufs_1jo","userId":"13927265952478603851"}},"outputId":"d8c57325-712c-43fc-a2ed-76393907551c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["200개의 텍스트를 토큰화했습니다.\n","400개의 텍스트를 토큰화했습니다.\n","600개의 텍스트를 토큰화했습니다.\n","800개의 텍스트를 토큰화했습니다.\n","200개의 텍스트를 토큰화했습니다.\n","토큰화 완료!\n"]}]},{"cell_type":"code","source":["# 학습할 때 tokenizer의 상태 저장하기\n","import pickle\n","with open('/content/drive/MyDrive/finalvoice/bi_lstm_attention_tokenizer.pickle', 'wb') as handle:\n","    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"],"metadata":{"id":"XUj0l-N7IRs2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 예측할 때 tokenizer의 상태 불러오기\n","with open('/content/drive/MyDrive/finalvoice/bi_lstm_attention_tokenizer.pickle', 'rb') as handle:\n","    tokenizer = pickle.load(handle)"],"metadata":{"id":"PCcmxuRvJczw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2 Attention + LSTM 모델\n"],"metadata":{"id":"r18pg_gpHD8O"}},{"cell_type":"markdown","source":["#### 2.1. 모델 설계"],"metadata":{"id":"Tzm6yCCWBfqW"}},{"cell_type":"markdown","source":["\n","2.1.1. Attention 매커니즘"],"metadata":{"id":"9_Q62ntdVlVz"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"j1GPAtwGVd_B"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.layers import Layer\n","\n","class AttentionLayer(Layer):\n","    def __init__(self, **kwargs):\n","        super(AttentionLayer, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        # Attention 가중치를 위한 weight 생성\n","        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1), initializer=\"normal\")\n","        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1), initializer=\"zeros\")\n","        super(AttentionLayer, self).build(input_shape)\n","\n","    def call(self, x):\n","        # Attention score 계산\n","        e = tf.keras.backend.tanh(tf.keras.backend.dot(x, self.W) + self.b)\n","        a = tf.keras.backend.softmax(e, axis=1)\n","        output = x * a\n","        return tf.keras.backend.sum(output, axis=1)\n","\n","    def compute_output_shape(self, input_shape):\n","        return (input_shape[0], input_shape[-1])\n"]},{"cell_type":"markdown","source":["2.1.2. LSTM 모델 설계"],"metadata":{"id":"XC_jqhRNZtS6"}},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# Tokenizer 객체 생성 및 학습\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(X_train_tokenized)\n","\n","# 텍스트 데이터를 숫자로 변환\n","X_train_sequences = tokenizer.texts_to_sequences(X_train_tokenized)\n","X_test_sequences = tokenizer.texts_to_sequences(X_test_tokenized)\n","\n","# 패딩 처리\n","MAX_LENGTH = max(len(s) for s in X_train_sequences)\n","X_train_padded = pad_sequences(X_train_sequences, maxlen=MAX_LENGTH)\n","X_test_padded = pad_sequences(X_test_sequences, maxlen=MAX_LENGTH)"],"metadata":{"id":"OIvQmrENrEYg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Concatenate, BatchNormalization, Input\n","\n","VOCAB_SIZE = len(tokenizer.word_index) + 1\n","EMBEDDING_DIM = 128\n","\n","# 모델 구성\n","input_layer = Input(shape=(MAX_LENGTH,))\n","embedding_layer = Embedding(VOCAB_SIZE, EMBEDDING_DIM)(input_layer)\n","bi_lstm = Bidirectional(LSTM(64, return_sequences=True))(embedding_layer)\n","attention = AttentionLayer()(bi_lstm)\n","output = Dense(1, activation='sigmoid')(attention)\n","\n","model = Model(inputs=input_layer, outputs=output)\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","model.summary()\n","\n","# 모델 학습\n","model.fit(X_train_padded, y_train, epochs=10, batch_size=64, validation_data=(X_test_padded, y_test))"],"metadata":{"id":"fUNq-U7OGw55","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692689153075,"user_tz":-540,"elapsed":31924,"user":{"displayName":"hufs_1jo","userId":"13927265952478603851"}},"outputId":"25c6649e-a0f0-4e58-c76d-705044b18964"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 100)]             0         \n","                                                                 \n"," embedding (Embedding)       (None, 100, 128)          1126912   \n","                                                                 \n"," bidirectional (Bidirectiona  (None, 100, 128)         98816     \n"," l)                                                              \n","                                                                 \n"," attention_layer (AttentionL  (None, 128)              228       \n"," ayer)                                                           \n","                                                                 \n"," dense (Dense)               (None, 1)                 129       \n","                                                                 \n","=================================================================\n","Total params: 1,226,085\n","Trainable params: 1,226,085\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/10\n","16/16 [==============================] - 9s 314ms/step - loss: 0.6889 - accuracy: 0.5380 - val_loss: 0.6300 - val_accuracy: 0.8320\n","Epoch 2/10\n","16/16 [==============================] - 4s 228ms/step - loss: 0.6601 - accuracy: 0.6253 - val_loss: 0.4673 - val_accuracy: 0.8852\n","Epoch 3/10\n","16/16 [==============================] - 3s 208ms/step - loss: 0.6062 - accuracy: 0.6591 - val_loss: 0.4613 - val_accuracy: 0.8975\n","Epoch 4/10\n","16/16 [==============================] - 2s 138ms/step - loss: 0.4913 - accuracy: 0.7536 - val_loss: 0.4020 - val_accuracy: 0.8156\n","Epoch 5/10\n","16/16 [==============================] - 3s 168ms/step - loss: 0.2680 - accuracy: 0.9025 - val_loss: 0.5366 - val_accuracy: 0.7623\n","Epoch 6/10\n","16/16 [==============================] - 3s 167ms/step - loss: 0.1489 - accuracy: 0.9528 - val_loss: 0.6779 - val_accuracy: 0.6230\n","Epoch 7/10\n","16/16 [==============================] - 2s 127ms/step - loss: 0.0906 - accuracy: 0.9805 - val_loss: 0.6149 - val_accuracy: 0.7172\n","Epoch 8/10\n","16/16 [==============================] - 2s 93ms/step - loss: 0.0558 - accuracy: 0.9846 - val_loss: 0.9058 - val_accuracy: 0.6516\n","Epoch 9/10\n","16/16 [==============================] - 2s 107ms/step - loss: 0.0391 - accuracy: 0.9897 - val_loss: 0.8637 - val_accuracy: 0.7213\n","Epoch 10/10\n","16/16 [==============================] - 1s 96ms/step - loss: 0.0291 - accuracy: 0.9928 - val_loss: 0.9498 - val_accuracy: 0.6967\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7b60d168c5b0>"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["model.save(\"/content/drive/MyDrive/finalvoice/bi_lstm_attention_model.h5\")"],"metadata":{"id":"4r5G4EyJulFE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 여기부터 돌리면됨"],"metadata":{"id":"5Fuf5mU-50Kf"}},{"cell_type":"code","source":["!pip install konlpy\n","import os\n","from konlpy.tag import Okt\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","okt = Okt()\n","# tokenizer = Tokenizer()\n","import pickle\n","# 예측할 때 tokenizer의 상태 불러오기\n","with open('/content/drive/MyDrive/finalvoice/bi_lstm_attention_tokenizer.pickle', 'rb') as handle:\n","    tokenizer = pickle.load(handle)\n","import tensorflow as tf\n","from tensorflow.keras.layers import Layer\n","# 모델 불러오기\n","from tensorflow.keras.models import load_model\n","model = load_model(\"/content/drive/MyDrive/finalvoice/bi_lstm_attention_model.h5\", custom_objects={'AttentionLayer': AttentionLayer})\n","\n","# 커스템 레이어 정의\n","class AttentionLayer(Layer):\n","    def __init__(self, **kwargs):\n","        super(AttentionLayer, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        # Attention 가중치를 위한 weight 생성\n","        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1), initializer=\"normal\")\n","        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1), initializer=\"zeros\")\n","        super(AttentionLayer, self).build(input_shape)\n","\n","    def call(self, x):\n","        # Attention score 계산\n","        e = tf.keras.backend.tanh(tf.keras.backend.dot(x, self.W) + self.b)\n","        a = tf.keras.backend.softmax(e, axis=1)\n","        output = x * a\n","        return tf.keras.backend.sum(output, axis=1)\n","\n","    def compute_output_shape(self, input_shape):\n","        return (input_shape[0], input_shape[-1])\n","\n","def predict_label_prob(sentence):\n","    # 문장 토큰화\n","    tokenized_sentence = okt.morphs(sentence)\n","    print(f\"토큰화된 문장: {tokenized_sentence}\")  # 추가\n","    # 토큰화된 문장을 숫자 시퀀스로 변환\n","    sequence = tokenizer.texts_to_sequences([tokenized_sentence])\n","    print(f\"시퀀스로 변환된 문장: {sequence}\")  # 추가\n","    # 패딩 처리\n","    MAX_LENGTH = 100\n","    padded_sequence = pad_sequences(sequence, maxlen=MAX_LENGTH)\n","    print(f\"패딩 처리된 문장: {padded_sequence}\")  # 추가\n","    # 예측 수행\n","    prediction = model.predict(padded_sequence)\n","    # 예측 확률 반환\n","    return prediction[0][0]\n","\n","# 사용자로부터 문장 입력 받기\n","user_input = input(\"문장을 입력하세요: \")\n","predicted_prob = predict_label_prob(user_input)\n","\n","print(f\"라벨 0의 확률: {1 - predicted_prob:.4f}\")\n","print(f\"라벨 1의 확률: {predicted_prob:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"saweSB-_5y7t","executionInfo":{"status":"ok","timestamp":1692689536859,"user_tz":-540,"elapsed":14628,"user":{"displayName":"hufs_1jo","userId":"13927265952478603851"}},"outputId":"160f0625-31d6-47b7-82e9-ce34de7ba761"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: konlpy in /usr/local/lib/python3.10/dist-packages (0.6.0)\n","Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.4.1)\n","Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.23.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.1)\n","문장을 입력하세요: 통장 주민 번호 및 사칭 계좌 번호\n","토큰화된 문장: ['통장', '주민', '번호', '및', '사칭', '계좌', '번호']\n","시퀀스로 변환된 문장: [[12, 408, 50, 2356, 19, 50]]\n","패딩 처리된 문장: [[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0   12  408   50 2356\n","    19   50]]\n","1/1 [==============================] - 1s 680ms/step\n","라벨 0의 확률: 0.1112\n","라벨 1의 확률: 0.8888\n"]}]},{"cell_type":"markdown","source":["#### 2.3. SAVE/LOAD"],"metadata":{"id":"Zite23EIbBHd"}},{"cell_type":"markdown","source":["2.3.1. save"],"metadata":{"id":"tWwSPV3sB-oV"}},{"cell_type":"code","source":["# 모델 저장\n","save_path_attention_lstm = '/content/drive/MyDrive/finalvoice/attention_lstm_model.h5'\n","model_attention_lstm.save(save_path_attention_lstm)\n","\n","print(f\"모델이 '{save_path_attention_lstm}' 경로에 저장되었습니다.\")"],"metadata":{"id":"iPZ70fsSAXlu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692680762768,"user_tz":-540,"elapsed":1604,"user":{"displayName":"hufs_1jo","userId":"13927265952478603851"}},"outputId":"01df0794-8e8b-46c0-a1c4-055cd5c46b06"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["모델이 '/content/drive/MyDrive/finalvoice/attention_lstm_model.h5' 경로에 저장되었습니다.\n"]}]},{"cell_type":"markdown","source":["2.3.2. load"],"metadata":{"id":"VEB9CpofbCzJ"}},{"cell_type":"code","source":["from tensorflow.keras.models import load_model\n","save_path_attention_lstm = '/content/drive/MyDrive/finalvoice/attention_lstm_model.h5'\n","# 저장한 모델 불러오기\n","model_attention_lstm = load_model(save_path_attention_lstm, custom_objects={\"AttentionLayer\": AttentionLayer})\n","\n","# 모델 요약 출력\n","model_attention_lstm.summary()\n","\n","print(f\"모델이 '{save_path_attention_lstm}' 경로에서 불러와졌습니다.\")"],"metadata":{"id":"Zx-y_rcka-8t","executionInfo":{"status":"error","timestamp":1692680807214,"user_tz":-540,"elapsed":3093,"user":{"displayName":"hufs_1jo","userId":"13927265952478603851"}},"colab":{"base_uri":"https://localhost:8080/","height":247},"outputId":"6a5931cd-d247-4997-938f-e970fb5e7f61"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-fb256d5a41f5>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msave_path_attention_lstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/finalvoice/attention_lstm_model.h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 저장한 모델 불러오기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel_attention_lstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path_attention_lstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"AttentionLayer\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAttentionLayer\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 모델 요약 출력\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'AttentionLayer' is not defined"]}]},{"cell_type":"markdown","source":["#### 2.4. 성능 평가"],"metadata":{"id":"hzlaC5RrcfJF"}},{"cell_type":"code","source":["loss, accuracy = model_attention_lstm.evaluate(X_test_padded, y_test, batch_size=32)\n","\n","print(f\"Attention + LSTM 모델의 테스트 손실: {loss:.4f}\")\n","print(f\"Attention + LSTM 모델의 테스트 정확도: {accuracy:.4f}\")"],"metadata":{"id":"Ao0Em4IN1C5j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3.  KoBERT 모델"],"metadata":{"id":"mhE_m4WPZ5aX"}},{"cell_type":"markdown","source":["#### 3.1.모델 구축"],"metadata":{"id":"m5Qg33nPYFyq"}},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"Vm2rMcbiT9cn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692691368155,"user_tz":-540,"elapsed":10634,"user":{"displayName":"hufs_1jo","userId":"13927265952478603851"}},"outputId":"29211da9-77a0-4dc7-b0d8-f400bb2e0d2e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m110.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.16.4 safetensors-0.3.2 tokenizers-0.13.3 transformers-4.31.0\n"]}]},{"cell_type":"code","source":["from transformers import BertTokenizer, TFBertForSequenceClassification\n","from tensorflow.keras.optimizers import Adam\n","\n","# 학습률 조정\n","optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n","# 모델 구성 및 컴파일\n","model_kobert.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n","model_kobert.summary()"],"metadata":{"id":"Wb-zSnqaYEo_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 3.2. 학습하기"],"metadata":{"id":"FUlE3wBzbePQ"}},{"cell_type":"code","source":["import tensorflow as tf\n","from transformers import BertTokenizer, TFBertForSequenceClassification\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.callbacks import EarlyStopping\n","import pandas as pd"],"metadata":{"id":"tHpCEIIEak8w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# KoBERT 토크나이저와 모델 로드\n","tokenizer_kobert = BertTokenizer.from_pretrained('monologg/kobert')\n","model_kobert = TFBertForSequenceClassification.from_pretrained('monologg/kobert', num_labels=2) # num_labels는 분류할 레이블의 수에 따라 조정하세요.\n","\n","# 하이퍼파라미터 설정\n","MAX_LENGTH = 128  # 원하는 문장 최대 길이 설정\n","\n","# KoBERT를 사용할 때, 입력 데이터는 토큰화 후 패딩 처리된 것이 아닌 원래의 텍스트 데이터를 사용해야 합니다.\n","train_encodings = tokenizer_kobert(list(X_train), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors=\"tf\")\n","test_encodings = tokenizer_kobert(list(X_test), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors=\"tf\")\n","\n","# Early Stopping을 사용하여 성능 향상이 없을 때 학습을 조기 종료하는 기법을 적용하였습니다.\n","early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n","\n","optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","metrics = ['accuracy']\n","\n","model_kobert.compile(optimizer=optimizer, loss=loss, metrics=metrics)"],"metadata":{"id":"SnujzzvWCKnM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델 학습\n","history_kobert = model_kobert.fit(\n","    [train_encodings['input_ids'], train_encodings['attention_mask']], y_train,\n","    epochs=12,\n","    batch_size=32,\n","    validation_data=([test_encodings['input_ids'], test_encodings['attention_mask']], y_test),\n","    callbacks=[early_stopping]\n",")"],"metadata":{"id":"tw2ruJwzZ5PF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 3.3. SAVE/LOAD"],"metadata":{"id":"PnpjXl0IbLVj"}},{"cell_type":"markdown","source":["save"],"metadata":{"id":"lecJfUvuCIv9"}},{"cell_type":"code","source":["# 모델 저장하기\n","save_path = ('/content/drive/MyDrive/finalmodel/model_kobert')\n","model_kobert.save_pretrained(save_path)\n","tokenizer_kobert.save_pretrained(save_path)\n","\n","print(f\"모델과 토크나이저가 '{save_path}' 경로에 저장되었습니다.\")"],"metadata":{"id":"ovvEjO6zbLVp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["load"],"metadata":{"id":"5Mwl5eRIbLVp"}},{"cell_type":"code","source":["from transformers import BertTokenizer, TFBertForSequenceClassification\n","\n","# 저장한 모델과 토크나이저 불러오기\n","loaded_tokenizer = BertTokenizer.from_pretrained('/content/drive/MyDrive/finalmodel/model_kobert')\n","loaded_model = TFBertForSequenceClassification.from_pretrained('/content/drive/MyDrive/finalmodel/model_kobert')\n","\n","print(\"모델과 토크나이저가 불러와졌습니다.\")"],"metadata":{"id":"PA1LaEqabLVp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["모델 초기화 (필요 시)"],"metadata":{"id":"fNJGOUSl7mGI"}},{"cell_type":"code","source":["from transformers import BertTokenizer, TFBertForSequenceClassification\n","\n","# 원본 모델과 토크나이저 로드\n","original_tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n","original_model = TFBertForSequenceClassification.from_pretrained('monologg/kobert', num_labels=2)\n","\n","# 로드한 모델의 가중치를 원본 모델의 가중치로 복사\n","loaded_model.set_weights(original_model.get_weights())\n","loaded_tokenizer = original_tokenizer\n","\n","print(\"모델과 토크나이저가 원본 모델에서 초기화되었습니다.\")"],"metadata":{"id":"39Q31wEsv82E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 3.4. 평가하기"],"metadata":{"id":"-ZQROZQyqR7-"}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score\n","\n","# 모델 컴파일\n","loaded_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","# 테스트 데이터의 인코딩\n","test_encodings = loaded_tokenizer(list(X_test), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors=\"tf\")\n","\n","# 모델 성능 평가\n","loss, accuracy = model_kobert.evaluate(\n","    [test_encodings['input_ids'], test_encodings['attention_mask']], y_test\n",")\n","\n","print(f\"테스트 데이터에 대한 손실: {loss:.4f}\")\n","print(f\"테스트 데이터에 대한 정확도: {accuracy:.4f}\")\n"],"metadata":{"id":"TLdA3HxBzBXU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4. 앙상블"],"metadata":{"id":"EgONcodDn2zY"}},{"cell_type":"markdown","source":["#### 소프트 보팅 방법 채택"],"metadata":{"id":"tE0cWVkDHJFd"}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, log_loss\n","\n","# 모델 컴파일\n","loaded_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","# 테스트 데이터의 인코딩\n","test_encodings = loaded_tokenizer(list(X_test), truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors=\"tf\")\n","\n","# 각 모델의 예측 결과 계산\n","prob_kobert = loaded_model.predict([test_encodings['input_ids'], test_encodings['attention_mask']]).logits\n","prob_attention_lstm = model_attention_lstm.predict(X_test_padded)\n","\n","# 로짓 값을 확률로 변환\n","probabilities_kobert = tf.nn.sigmoid(prob_kobert)\n","probabilities_attention_lstm = tf.nn.sigmoid(prob_attention_lstm)\n","\n","# 앙상블을 위한 예측 확률 계산\n","ensemble_probabilities = (probabilities_kobert + probabilities_attention_lstm) / 2\n","\n","# 앙상블 결과를 소프트 보팅하여 예측 클래스 계산\n","ensemble_predicted_classes = np.argmax(ensemble_probabilities, axis=1)\n","\n","# 정확도 평가\n","ensemble_accuracy = accuracy_score(y_test, ensemble_predicted_classes)\n","print(f\"모델 앙상블의 예측 정확도: {ensemble_accuracy:.4f}\")\n","\n","# 로스 평가\n","ensemble_loss = log_loss(y_test, ensemble_probabilities)\n","print(f\"모델 앙상블의 로스: {ensemble_loss:.4f}\")"],"metadata":{"id":"OWjYnHsjrctu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5. 최종 평가"],"metadata":{"id":"XtR9wtstn-cX"}},{"cell_type":"markdown","source":["#### 5.1. 모델 평가"],"metadata":{"id":"xA3VC_1xD6UO"}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","# 정확도 평가\n","ensemble_accuracy = accuracy_score(y_test, ensemble_predicted_classes)\n","print(f\"앙상블 모델의 예측 정확도: {ensemble_accuracy:.4f}\")\n","\n","# 정밀도 평가\n","precision = precision_score(y_test, ensemble_predicted_classes)\n","print(f\"앙상블 모델의 정밀도: {precision:.4f}\")\n","\n","# 재현율 평가\n","recall = recall_score(y_test, ensemble_predicted_classes)\n","print(f\"앙상블 모델의 재현율: {recall:.4f}\")\n","\n","# F1 점수 평가\n","f1 = f1_score(y_test, ensemble_predicted_classes)\n","print(f\"앙상블 모델의 F1 점수: {f1:.4f}\")"],"metadata":{"id":"mz660PsDn-FC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 5.2. I/O **테스트**"],"metadata":{"id":"DGZ8AyXPCfgP"}},{"cell_type":"code","source":["from transformers import BertTokenizer, TFBertForSequenceClassification\n","import numpy as np\n","import tensorflow as tf\n","\n","# 불러온 모델 및 토크나이저 설정\n","loaded_tokenizer = BertTokenizer.from_pretrained('/content/drive/MyDrive/finalmodel/model_kobert')\n","loaded_model = TFBertForSequenceClassification.from_pretrained('/content/drive/MyDrive/finalmodel/model_kobert')\n","\n","# 텍스트 입력 받기\n","text = input(\"예측할 텍스트를 입력하세요: \")\n","\n","# 입력 텍스트의 전처리 및 인코딩\n","encoded_input = loaded_tokenizer(text, truncation=True, padding=True, max_length=128, return_tensors=\"tf\")\n","\n","# 각 모델의 예측 결과 계산\n","prob_kobert = loaded_model.predict([encoded_input['input_ids'], encoded_input['attention_mask']]).logits\n","prob_attention_lstm = model_attention_lstm.predict(X_test_padded)\n","\n","# 로짓 값을 확률로 변환\n","probabilities_kobert = tf.nn.sigmoid(prob_kobert)\n","probabilities_attention_lstm = tf.nn.sigmoid(prob_attention_lstm)\n","\n","# 앙상블을 위한 예측 확률 계산\n","ensemble_probabilities = (probabilities_kobert + probabilities_attention_lstm) / 2\n","\n","# 앙상블 결과를 소프트 보팅하여 예측 클래스 계산\n","ensemble_predicted_class = np.argmax(ensemble_probabilities, axis=1)[0]\n","\n","# 예측 클래스와 확률 출력\n","class_names = ['정상', '스팸']  # 클래스 이름 설정\n","predicted_class_name = class_names[ensemble_predicted_class]\n","predicted_class_probability = ensemble_probabilities[0][ensemble_predicted_class]\n","print(f\"예측: {predicted_class_name}\")\n","print(f\"확률: {predicted_class_probability:.4f}\")"],"metadata":{"id":"T_ss1gCm3Lmf"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1cY7fqqzUghF1e4k4duNKuPKLg557SI45","timestamp":1692679428059}],"machine_shape":"hm","gpuType":"T4","mount_file_id":"1pdI9cIgk2IXsWvrxvaDwGLdvfR7BxXXV","authorship_tag":"ABX9TyPu0cHeGGCeRputGtbAPHHF"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}