{"cells":[{"cell_type":"markdown","source":["### 필요한 라이브러리"],"metadata":{"id":"-gX6P6Hm2zGX"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6GNQjBL4SXYz"},"outputs":[],"source":["!pip install transformers\n","!pip install accelerate>=0.20.1\n","!pip install transformers[torch]\n","!pip install accelerate transformers[torch]\n","!pip install gradio"]},{"cell_type":"code","source":["import pandas as pd\n","import torch\n","import transformers\n","import gradio as gr\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import DataLoader, TensorDataset\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n","from tqdm import tqdm\n","from google.colab import drive\n","drive.mount('/content/drive')\n","import pandas as pd\n","from sklearn.utils import shuffle"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a7DwW1ZFUCVv","executionInfo":{"status":"ok","timestamp":1692902657796,"user_tz":-540,"elapsed":15622,"user":{"displayName":"hufs_1jo","userId":"13927265952478603851"}},"outputId":"4fcb2d4d-1e8a-4753-df34-a4ecd7bc8c46"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# 데이터 불러오기\n","label1_data = pd.read_csv(\"/content/drive/MyDrive/voise_antinoise.csv\",encoding='euc-kr')\n","label0_data_1 = pd.read_csv(\"/content/drive/MyDrive/normalcall_original.csv\")\n","label0_data_2 = pd.read_csv(\"/content/drive/MyDrive/finaldataset/nofish.csv\")\n","label1_data[\"text\"] = label1_data[\"Sentence\"]\n","label1_data.drop(columns=['Sentence'], inplace=True)\n","label0_data_2[\"text\"] = label0_data_2[\"refined_text\"]\n","label0_data_2.drop(columns=['refined_text'], inplace=True)\n","\n","# 라벨 0 데이터 랜덤 추출\n","label0_data_1_sampled = label0_data_1.sample(n=1204, random_state=42)\n","label0_data_2_sampled = label0_data_2.sample(n=1200, random_state=42)\n","\n","# 라벨 부여\n","label1_data['label'] = 1\n","label0_data_1_sampled['label'] = 0\n","label0_data_2_sampled['label'] = 0\n","\n","# 데이터 병합\n","intergrated_unbalan5 = pd.concat([label1_data, label0_data_1_sampled, label0_data_2_sampled])\n","intergrated_unbalan5 = shuffle(intergrated_unbalan5, random_state=42).reset_index(drop=True)"],"metadata":{"id":"UbG5UC44Kum2"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bC_S6r6zVbkB"},"outputs":[],"source":["intergrated_unbalan5.to_csv(\"/content/drive/MyDrive/intergrated_unbalan5.csv\", index = False)"]},{"cell_type":"markdown","source":["### KoBERT 2천건"],"metadata":{"id":"R8g6qlnZ2bqJ"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":3800722,"status":"ok","timestamp":1692888086735,"user":{"displayName":"hufs_1jo","userId":"13927265952478603851"},"user_tz":-540},"id":"iHdCsnC5SDeB","outputId":"b045d215-2c31-468d-e69b-cd9fff3dfde6"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Epoch 1/5 - Training: 100%|██████████| 140/140 [11:54<00:00,  5.11s/it]\n","Epoch 1/5 - Validation: 100%|██████████| 35/35 [00:57<00:00,  1.63s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/5: Val Loss: 0.0593, Val Acc: 0.9875\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/5 - Training: 100%|██████████| 140/140 [11:44<00:00,  5.03s/it]\n","Epoch 2/5 - Validation: 100%|██████████| 35/35 [00:59<00:00,  1.69s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2/5: Val Loss: 0.0648, Val Acc: 0.9804\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/5 - Training: 100%|██████████| 140/140 [11:35<00:00,  4.97s/it]\n","Epoch 3/5 - Validation: 100%|██████████| 35/35 [00:56<00:00,  1.61s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3/5: Val Loss: 0.0243, Val Acc: 0.9964\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/5 - Training: 100%|██████████| 140/140 [11:33<00:00,  4.96s/it]\n","Epoch 4/5 - Validation: 100%|██████████| 35/35 [00:54<00:00,  1.57s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4/5: Val Loss: 0.0437, Val Acc: 0.9893\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5/5 - Training: 100%|██████████| 140/140 [11:31<00:00,  4.94s/it]\n","Epoch 5/5 - Validation: 100%|██████████| 35/35 [00:56<00:00,  1.63s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5/5: Val Loss: 0.0377, Val Acc: 0.9875\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-17-d0cf47f7d199>:100: GradioDeprecationWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n","  iface = gr.Interface(fn=predict, inputs=\"text\", outputs=gr.outputs.JSON())\n"]},{"output_type":"stream","name":"stdout","text":["Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n","\n","To create a public link, set `share=True` in `launch()`.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["(async (port, path, width, height, cache, element) => {\n","                        if (!google.colab.kernel.accessAllowed && !cache) {\n","                            return;\n","                        }\n","                        element.appendChild(document.createTextNode(''));\n","                        const url = await google.colab.kernel.proxyPort(port, {cache});\n","\n","                        const external_link = document.createElement('div');\n","                        external_link.innerHTML = `\n","                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n","                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n","                                    https://localhost:${port}${path}\n","                                </a>\n","                            </div>\n","                        `;\n","                        element.appendChild(external_link);\n","\n","                        const iframe = document.createElement('iframe');\n","                        iframe.src = new URL(path, url).toString();\n","                        iframe.height = height;\n","                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n","                        iframe.width = width;\n","                        iframe.style.border = 0;\n","                        element.appendChild(iframe);\n","                    })(7860, \"/\", \"100%\", 500, false, window.element)"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":17}],"source":["|# 데이터 불러오기\n","data = pd.read_csv(\"/content/drive/MyDrive/intergrated_unbalan5.csv\")\n","\n","# 텍스트와 라벨 컬럼 분리\n","texts = data['text'].tolist()\n","labels = data['label'].tolist()\n","\n","# 훈련 데이터와 검증 데이터 분리\n","train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n","\n","# KoBERT 토크나이저 및 모델 불러오기\n","tokenizer = AutoTokenizer.from_pretrained(\"monologg/kobert\", use_fast=False)\n","model = AutoModelForSequenceClassification.from_pretrained(\"monologg/kobert\", num_labels=2)\n","\n","# 토큰화 및 패딩\n","max_length = 128  # 적절한 시퀀스 길이 설정\n","train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n","val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n","\n","# PyTorch 데이터셋 생성\n","train_dataset = TensorDataset(train_encodings['input_ids'],\n","                              train_encodings['attention_mask'],\n","                              torch.tensor(train_labels))\n","val_dataset = TensorDataset(val_encodings['input_ids'],\n","                            val_encodings['attention_mask'],\n","                            torch.tensor(val_labels))\n","\n","# 데이터 로더 생성\n","batch_size = 16  # 적절한 배치 크기 설정\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","\n","# 모델 및 옵티마이저 설정\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","\n","# 학습\n","num_epochs = 5  # 적절한 에폭 수 설정\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n","        input_ids, attention_mask, labels = batch\n","        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","\n","    # 검증\n","    model.eval()\n","    val_loss = 0\n","    correct_predictions = 0\n","    total_predictions = 0\n","    with torch.no_grad():\n","        for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n","            input_ids, attention_mask, labels = batch\n","            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","\n","            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","            val_loss += outputs.loss.item()\n","\n","            predicted_labels = outputs.logits.argmax(dim=1)\n","            correct_predictions += (predicted_labels == labels).sum().item()\n","            total_predictions += labels.size(0)\n","\n","    val_accuracy = correct_predictions / total_predictions\n","    avg_val_loss = val_loss / len(val_loader)\n","    print(f\"Epoch {epoch+1}/{num_epochs}: Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n","\n","\n","\n","# 예측 함수 정의\n","def predict(text):\n","    encoding = tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\")\n","    encoding = {k: v.to(device) for k, v in encoding.items()}\n","\n","    with torch.no_grad():\n","        outputs = model(**encoding)\n","        logits = outputs.logits\n","        probabilities = torch.softmax(logits, dim=1).tolist()[0]\n","\n","    return {str(label): prob for label, prob in enumerate(probabilities)}\n","\n","# Gradio 인터페이스 정의\n","iface = gr.Interface(fn=predict, inputs=\"text\", outputs=gr.outputs.JSON())\n","iface.launch()\n"]},{"cell_type":"code","source":["# 학습된 모델 저장\n","model_save_path = \"/content/drive/MyDrive/intergrated_unbalan5\"\n","model.save_pretrained(model_save_path)"],"metadata":{"id":"_9rBM04OFTyc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### KoELECTRA 2천건 -1\n","과적합"],"metadata":{"id":"JqMXNQeD2io1"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"oyOGewbpTkPh"},"outputs":[],"source":["import torch\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import DataLoader, TensorDataset\n","from tqdm import tqdm\n","from transformers import ElectraTokenizer, ElectraForSequenceClassification, AdamW\n","import gradio as gr\n","\n","# Load data\n","data = pd.read_csv(\"/content/drive/MyDrive/intergrated_unbalan5.csv\")\n","\n","# Separate text and label columns\n","texts = data['text'].tolist()\n","labels = data['label'].tolist()\n","\n","# Separate training and validation data\n","train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n","\n","# Load the KoELECTRA tokenizer and model\n","tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-discriminator\", use_fast=False)\n","model = ElectraForSequenceClassification.from_pretrained(\"monologg/koelectra-base-discriminator\", num_labels=2)\n","\n","# Tokenization and padding\n","max_length = 128\n","train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n","val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n","\n","# Create a PyTorch dataset\n","train_dataset = TensorDataset(train_encodings['input_ids'],\n","                              train_encodings['attention_mask'],\n","                              torch.tensor(train_labels))\n","val_dataset = TensorDataset(val_encodings['input_ids'],\n","                            val_encodings['attention_mask'],\n","                            torch.tensor(val_labels))\n","\n","# Create data loader\n","batch_size = 16\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","\n","# Model and optimizer settings\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","\n","# Learning\n","num_epochs = 3\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n","        input_ids, attention_mask, labels = batch\n","        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    correct_predictions = 0\n","    total_predictions = 0\n","    with torch.no_grad():\n","        for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n","            input_ids, attention_mask, labels = batch\n","            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","\n","            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","            val_loss += outputs.loss.item()\n","\n","            predicted_labels = outputs.logits.argmax(dim=1)\n","            correct_predictions += (predicted_labels == labels).sum().item()\n","            total_predictions += labels.size(0)\n","\n","    val_accuracy = correct_predictions / total_predictions\n","    avg_val_loss = val_loss / len(val_loader)\n","    print(f\"Epoch {epoch+1}/{num_epochs}: Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n","\n","# Define the prediction function\n","def predict(text):\n","    encoding = tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\")\n","    encoding = {k: v.to(device) for k, v in encoding.items()}\n","\n","    with torch.no_grad():\n","        outputs = model(**encoding)\n","        logits = outputs.logits\n","        probabilities = torch.softmax(logits, dim=1).tolist()[0]\n","\n","    return {str(label): prob for label, prob in enumerate(probabilities)}\n","\n","# Define the Gradio interface\n","iface = gr.Interface(fn=predict, inputs=\"text\", outputs=gr.outputs.JSON())\n","iface.launch()"]},{"cell_type":"markdown","source":["### KoELECTRA 2천건 -2\n","가중치 규제 weight_decay=0.01\n","\n"],"metadata":{"id":"BaGl83Hr8rof"}},{"cell_type":"code","source":["import torch\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import DataLoader, TensorDataset\n","from tqdm import tqdm\n","from transformers import ElectraTokenizer, ElectraForSequenceClassification, AdamW\n","import gradio as gr\n","\n","# Load data\n","data = pd.read_csv(\"/content/drive/MyDrive/intergrated_unbalan5.csv\")\n","\n","# Separate text and label columns\n","texts = data['text'].tolist()\n","labels = data['label'].tolist()\n","\n","# Separate training and validation data\n","train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n","\n","# Load the KoELECTRA tokenizer and model\n","tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-discriminator\", use_fast=False)\n","model = ElectraForSequenceClassification.from_pretrained(\"monologg/koelectra-base-discriminator\", num_labels=2)\n","\n","# Tokenization and padding\n","max_length = 128\n","train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n","val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n","\n","# Create a PyTorch dataset\n","train_dataset = TensorDataset(train_encodings['input_ids'],\n","                              train_encodings['attention_mask'],\n","                              torch.tensor(train_labels))\n","val_dataset = TensorDataset(val_encodings['input_ids'],\n","                            val_encodings['attention_mask'],\n","                            torch.tensor(val_labels))\n","\n","# Create data loader\n","batch_size = 16\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","\n","# Model and optimizer settings\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n","\n","# Learning rate scheduling\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.8)\n","\n","# Learning\n","num_epochs = 3\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n","        input_ids, attention_mask, labels = batch\n","        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    correct_predictions = 0\n","    total_predictions = 0\n","    with torch.no_grad():\n","        for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n","            input_ids, attention_mask, labels = batch\n","            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","\n","            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","            val_loss += outputs.loss.item()\n","\n","            predicted_labels = outputs.logits.argmax(dim=1)\n","            correct_predictions += (predicted_labels == labels).sum().item()\n","            total_predictions += labels.size(0)\n","\n","    val_accuracy = correct_predictions / total_predictions\n","    avg_val_loss = val_loss / len(val_loader)\n","    print(f\"Epoch {epoch+1}/{num_epochs}: Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n","\n","    # Learning rate scheduling step\n","    scheduler.step()\n","\n","# Define the prediction function (same as before)\n","def predict(text):\n","    encoding = tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\")\n","    encoding = {k: v.to(device) for k, v in encoding.items()}\n","\n","    with torch.no_grad():\n","        outputs = model(**encoding)\n","        logits = outputs.logits\n","        probabilities = torch.softmax(logits, dim=1).tolist()[0]\n","\n","    return {str(label): prob for label, prob in enumerate(probabilities)}\n","\n","# Define the Gradio interface (same as before)\n","iface = gr.Interface(fn=predict, inputs=\"text\", outputs=gr.outputs.JSON())\n","iface.launch()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":897},"id":"cd_GsLLw7vmO","executionInfo":{"status":"ok","timestamp":1692904197666,"user_tz":-540,"elapsed":43015,"user":{"displayName":"정희수","userId":"12757347728745877861"}},"outputId":"786e75ac-557a-431d-9926-a774c5995888"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Epoch 1/3 - Training: 100%|██████████| 140/140 [00:11<00:00, 11.74it/s]\n","Epoch 1/3 - Validation: 100%|██████████| 35/35 [00:00<00:00, 36.21it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/3: Val Loss: 0.0229, Val Acc: 0.9929\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/3 - Training: 100%|██████████| 140/140 [00:11<00:00, 11.77it/s]\n","Epoch 2/3 - Validation: 100%|██████████| 35/35 [00:00<00:00, 36.11it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2/3: Val Loss: 0.0024, Val Acc: 1.0000\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/3 - Training: 100%|██████████| 140/140 [00:11<00:00, 11.79it/s]\n","Epoch 3/3 - Validation: 100%|██████████| 35/35 [00:00<00:00, 36.37it/s]\n","<ipython-input-10-1c3900e74eac>:101: GradioDeprecationWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n","  iface = gr.Interface(fn=predict, inputs=\"text\", outputs=gr.outputs.JSON())\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3/3: Val Loss: 0.0053, Val Acc: 0.9982\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n","\n","To create a public link, set `share=True` in `launch()`.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["(async (port, path, width, height, cache, element) => {\n","                        if (!google.colab.kernel.accessAllowed && !cache) {\n","                            return;\n","                        }\n","                        element.appendChild(document.createTextNode(''));\n","                        const url = await google.colab.kernel.proxyPort(port, {cache});\n","\n","                        const external_link = document.createElement('div');\n","                        external_link.innerHTML = `\n","                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n","                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n","                                    https://localhost:${port}${path}\n","                                </a>\n","                            </div>\n","                        `;\n","                        element.appendChild(external_link);\n","\n","                        const iframe = document.createElement('iframe');\n","                        iframe.src = new URL(path, url).toString();\n","                        iframe.height = height;\n","                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n","                        iframe.width = width;\n","                        iframe.style.border = 0;\n","                        element.appendChild(iframe);\n","                    })(7862, \"/\", \"100%\", 500, false, window.element)"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":10}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LRDazBziVEBE"},"outputs":[],"source":["# 학습된 모델 저장\n","model_save_path = \"/content/drive/MyDrive/KoELECTRA_2K\"\n","model.save_pretrained(model_save_path)"]},{"cell_type":"markdown","source":["### KoGPT - FAIL"],"metadata":{"id":"VCdskC3T_VUf"}},{"cell_type":"code","source":["import torch\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import DataLoader, TensorDataset\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW\n","import gradio as gr\n","\n","# Load data\n","data = pd.read_csv(\"/content/drive/MyDrive/integrated_unbalan5.csv\")\n","\n","# Separate text and label columns\n","texts = data['text'].tolist()\n","labels = data['label'].tolist()\n","\n","# Separate training and validation data\n","train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n","\n","# Load the KoGPT-2 tokenizer and model\n","tokenizer = GPT2Tokenizer.from_pretrained(\"skt/kogpt2-base-v2\")\n","model = GPT2LMHeadModel.from_pretrained(\"skt/kogpt2-base-v2\")\n","\n","# Tokenization and padding\n","max_length = 128\n","train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n","val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n","\n","# Create a PyTorch dataset\n","train_dataset = TensorDataset(train_encodings['input_ids'],\n","                              train_encodings['attention_mask'],\n","                              torch.tensor(train_labels))\n","val_dataset = TensorDataset(val_encodings['input_ids'],\n","                            val_encodings['attention_mask'],\n","                            torch.tensor(val_labels))\n","\n","# Create data loader\n","batch_size = 16\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","\n","# Model and optimizer settings\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n","\n","# Learning\n","num_epochs = 3\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    for batch in train_loader:\n","        input_ids, attention_mask, labels = batch\n","        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","\n","# Save the trained model\n","save_path = \"/content/drive/MyDrive/KoGPT_2K\"\n","model.save_pretrained(save_path)\n","\n","# Define the prediction function\n","def predict(text):\n","    input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n","    with torch.no_grad():\n","        output = model.generate(input_ids, max_length=100, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n","    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n","    return generated_text\n","\n","# Define the Gradio interface\n","iface = gr.Interface(fn=predict, inputs=\"text\", outputs=\"text\")\n","iface.launch()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":404},"id":"pd0baj0__UGL","executionInfo":{"status":"error","timestamp":1692906920749,"user_tz":-540,"elapsed":2048,"user":{"displayName":"정희수","userId":"12757347728745877861"}},"outputId":"2a44ea14-89a4-4139-ed50-566712ea2e64"},"execution_count":null,"outputs":[{"output_type":"error","ename":"OSError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-f6f4a461dd00>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Load the KoGPT-2 tokenizer and model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2Tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"skt/kogpt2-base-v2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2LMHeadModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"skt/kogpt2-base-v2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1837\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_file_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfull_file_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1838\u001b[0;31m             raise EnvironmentError(\n\u001b[0m\u001b[1;32m   1839\u001b[0m                 \u001b[0;34mf\"Can't load tokenizer for '{pretrained_model_name_or_path}'. If you were trying to load it from \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1840\u001b[0m                 \u001b[0;34m\"'https://huggingface.co/models', make sure you don't have a local directory with the same name. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'skt/kogpt2-base-v2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'skt/kogpt2-base-v2' is the correct path to a directory containing all relevant files for a GPT2Tokenizer tokenizer."]}]},{"cell_type":"markdown","source":["### KoAlpaca - FAIL"],"metadata":{"id":"tSsCWbylIZj2"}},{"cell_type":"code","source":["import torch\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import DataLoader, TensorDataset\n","from tqdm import tqdm\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n","import gradio as gr\n","\n","# Load data\n","data = pd.read_csv(\"/content/drive/MyDrive/integrated_unbalan5.csv\")\n","\n","# Separate text and label columns\n","texts = data['text'].tolist()\n","labels = data['label'].tolist()\n","\n","# Separate training and validation data\n","train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n","\n","# Load the KoAlpaca tokenizer and model with pretrained weights\n","tokenizer = AutoTokenizer.from_pretrained(\"monologg/koelectra-base-v2-discriminator\", use_fast=False)\n","model = AutoModelForSequenceClassification.from_pretrained(\"monologg/koelectra-base-v2-discriminator\", num_labels=2)\n","\n","# Tokenization and padding\n","max_length = 128\n","train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n","val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n","\n","# Create a PyTorch dataset\n","train_dataset = TensorDataset(train_encodings['input_ids'],\n","                              train_encodings['attention_mask'],\n","                              torch.tensor(train_labels))\n","val_dataset = TensorDataset(val_encodings['input_ids'],\n","                            val_encodings['attention_mask'],\n","                            torch.tensor(val_labels))\n","\n","# Create data loader\n","batch_size = 16\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","\n","# Model and optimizer settings\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n","\n","# Learning\n","num_epochs = 3\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n","        input_ids, attention_mask, labels = batch\n","        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    correct_predictions = 0\n","    total_predictions = 0\n","    with torch.no_grad():\n","        for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n","            input_ids, attention_mask, labels = batch\n","            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","\n","            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","            val_loss += outputs.loss.item()\n","\n","            predicted_labels = outputs.logits.argmax(dim=1)\n","            correct_predictions += (predicted_labels == labels).sum().item()\n","            total_predictions += labels.size(0)\n","\n","    val_accuracy = correct_predictions / total_predictions\n","    avg_val_loss = val_loss / len(val_loader)\n","    print(f\"Epoch {epoch+1}/{num_epochs}: Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n","\n","# Save the model\n","model.save_pretrained(\"/content/drive/MyDrive/KoAlpaca_2K\")\n","\n","# Define the prediction function\n","def predict(text):\n","    encoding = tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\")\n","    encoding = {k: v.to(device) for k, v in encoding.items()}\n","\n","    with torch.no_grad():\n","        outputs = model(**encoding)\n","        logits = outputs.logits\n","        probabilities = torch.softmax(logits, dim=1).tolist()[0]\n","\n","    return {str(label): prob for label, prob in enumerate(probabilities)}\n","\n","# Define the Gradio interface\n","iface = gr.Interface(fn=predict, inputs=\"text\", outputs=gr.outputs.JSON())\n","iface.launch()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":500},"id":"bTd095OCGGZZ","executionInfo":{"status":"error","timestamp":1692907295893,"user_tz":-540,"elapsed":5014,"user":{"displayName":"정희수","userId":"12757347728745877861"}},"outputId":"3bf435f2-8c86-4e28-e651-1030be8814f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v2-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-fc2c9fb27b56>\u001b[0m in \u001b[0;36m<cell line: 43>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# Model and optimizer settings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2012\u001b[0m             )\n\u001b[1;32m   2013\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2014\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2016\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1143\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m     def register_full_backward_pre_hook(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m   1142\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m-> 1143\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}]},{"cell_type":"markdown","source":["### ATTBILSTM\n","DROPOUT 적용 (Accuracy가 0.85로 고정되는 문제 발생)\n"],"metadata":{"id":"EVtEd6L5PiwJ"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import DataLoader, TensorDataset\n","from tqdm import tqdm\n","from transformers import ElectraTokenizer, AdamW\n","import gradio as gr\n","\n","# Define Attention Layer\n","class Attention(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(Attention, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.W = nn.Linear(hidden_size, hidden_size)\n","        self.v = nn.Linear(hidden_size, 1, bias=False)\n","\n","    def forward(self, encoder_outputs):\n","        energy = torch.tanh(self.W(encoder_outputs))\n","        attention_scores = self.v(energy).squeeze(2)\n","        attention_weights = torch.softmax(attention_scores, dim=1)\n","        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n","        return context\n","\n","# Define Bi-LSTM with Attention\n","class BiLSTMWithAttention(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_classes, dropout_rate):\n","        super(BiLSTMWithAttention, self).__init__()\n","        self.embedding = nn.Embedding(input_size, hidden_size)\n","        self.dropout = nn.Dropout(dropout_rate)\n","        self.lstm = nn.LSTM(hidden_size, hidden_size, bidirectional=True)\n","        self.attention = Attention(hidden_size * 2)\n","        self.fc = nn.Linear(hidden_size * 2, num_classes)\n","\n","    def forward(self, x):\n","        embedded = self.dropout(self.embedding(x))\n","        lstm_out, _ = self.lstm(embedded)\n","        context = self.attention(lstm_out)\n","        output = self.fc(context)\n","        return output\n","\n","# Load data\n","data = pd.read_csv(\"/content/drive/MyDrive/intergrated_unbalan5.csv\")\n","\n","# Separate text and label columns\n","texts = data['text'].tolist()\n","labels = data['label'].tolist()\n","\n","# Separate training and validation data\n","train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n","\n","# Load the Electra tokenizer\n","tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-discriminator\", use_fast=False)\n","\n","# Tokenization and padding\n","max_length = 128\n","train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n","val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n","\n","# Create a PyTorch dataset\n","train_dataset = TensorDataset(train_encodings['input_ids'],\n","                              train_encodings['attention_mask'],\n","                              torch.tensor(train_labels))\n","val_dataset = TensorDataset(val_encodings['input_ids'],\n","                            val_encodings['attention_mask'],\n","                            torch.tensor(val_labels))\n","\n","# Create data loader\n","batch_size = 16\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","\n","# Model settings\n","input_size = len(tokenizer)\n","hidden_size = 256\n","num_classes = 2\n","dropout_rate = 0.5\n","learning_rate = 2e-5\n","\n","model = BiLSTMWithAttention(input_size, hidden_size, num_classes, dropout_rate)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","optimizer = AdamW(model.parameters(), lr=learning_rate)\n","\n","# Learning\n","num_epochs = 10\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n","        input_ids, attention_mask, labels = batch\n","        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(input_ids)\n","        loss_fn = nn.CrossEntropyLoss()\n","        loss = loss_fn(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    correct_predictions = 0\n","    total_predictions = 0\n","    with torch.no_grad():\n","        for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n","            input_ids, attention_mask, labels = batch\n","            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","\n","            outputs = model(input_ids)\n","            val_loss += loss_fn(outputs, labels).item()\n","\n","            predicted_labels = outputs.argmax(dim=1)\n","            correct_predictions += (predicted_labels == labels).sum().item()\n","            total_predictions += labels.size(0)\n","\n","    val_accuracy = correct_predictions / total_predictions\n","    # Calculate average loss\n","    avg_train_loss = total_loss / len(train_loader)\n","    print(f\"Epoch {epoch+1}/{num_epochs}: Avg Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n","\n","# Save the trained model\n","model_save_path = \"/content/drive/MyDrive/AttBiLSTM_2K\"\n","torch.save(model.state_dict(), model_save_path)\n","\n","# Define the prediction function\n","def predict(text):\n","    encoding = tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\")\n","    input_ids = encoding['input_ids'].to(device)\n","    with torch.no_grad():\n","        outputs = model(input_ids)\n","        probabilities = torch.softmax(outputs, dim=1).tolist()[0]\n","\n","    return {str(label): prob for label, prob in enumerate(probabilities)}\n","\n","# Define the Gradio interface\n","iface = gr.Interface(fn=predict, inputs=\"text\", outputs=gr.outputs.JSON())\n","iface.launch()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1692908857958,"user_tz":-540,"elapsed":754483,"user":{"displayName":"hufs_1jo","userId":"13927265952478603851"}},"outputId":"d93fd9a7-1f73-4860-e102-c295fce13afd","id":"Q8jUuL1MPiwJ"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Epoch 1/10 - Training: 100%|██████████| 140/140 [01:12<00:00,  1.93it/s]\n","Epoch 1/10 - Validation: 100%|██████████| 35/35 [00:04<00:00,  8.06it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10: Val Loss: 0.4844, Val Acc: 0.8500\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/10 - Training: 100%|██████████| 140/140 [01:25<00:00,  1.63it/s]\n","Epoch 2/10 - Validation: 100%|██████████| 35/35 [00:05<00:00,  5.91it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2/10: Val Loss: 0.4629, Val Acc: 0.8500\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/10 - Training: 100%|██████████| 140/140 [01:08<00:00,  2.05it/s]\n","Epoch 3/10 - Validation: 100%|██████████| 35/35 [00:05<00:00,  6.55it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3/10: Val Loss: 0.4367, Val Acc: 0.8500\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/10 - Training: 100%|██████████| 140/140 [01:05<00:00,  2.14it/s]\n","Epoch 4/10 - Validation: 100%|██████████| 35/35 [00:04<00:00,  8.09it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4/10: Val Loss: 0.4310, Val Acc: 0.8500\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5/10 - Training: 100%|██████████| 140/140 [01:05<00:00,  2.13it/s]\n","Epoch 5/10 - Validation: 100%|██████████| 35/35 [00:04<00:00,  8.06it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5/10: Val Loss: 0.4200, Val Acc: 0.8500\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6/10 - Training: 100%|██████████| 140/140 [01:08<00:00,  2.05it/s]\n","Epoch 6/10 - Validation: 100%|██████████| 35/35 [00:05<00:00,  6.36it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6/10: Val Loss: 0.4187, Val Acc: 0.8500\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 7/10 - Training: 100%|██████████| 140/140 [01:05<00:00,  2.12it/s]\n","Epoch 7/10 - Validation: 100%|██████████| 35/35 [00:05<00:00,  6.05it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7/10: Val Loss: 0.4082, Val Acc: 0.8500\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 8/10 - Training: 100%|██████████| 140/140 [01:08<00:00,  2.04it/s]\n","Epoch 8/10 - Validation: 100%|██████████| 35/35 [00:04<00:00,  8.04it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 8/10: Val Loss: 0.4088, Val Acc: 0.8500\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 9/10 - Training: 100%|██████████| 140/140 [01:05<00:00,  2.12it/s]\n","Epoch 9/10 - Validation: 100%|██████████| 35/35 [00:04<00:00,  7.32it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9/10: Val Loss: 0.4038, Val Acc: 0.8500\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 10/10 - Training: 100%|██████████| 140/140 [01:05<00:00,  2.13it/s]\n","Epoch 10/10 - Validation: 100%|██████████| 35/35 [00:06<00:00,  5.30it/s]\n","<ipython-input-24-0538c6f20703>:138: GradioDeprecationWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n","  iface = gr.Interface(fn=predict, inputs=\"text\", outputs=gr.outputs.JSON())\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 10/10: Val Loss: 0.4039, Val Acc: 0.8500\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n","\n","To create a public link, set `share=True` in `launch()`.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["(async (port, path, width, height, cache, element) => {\n","                        if (!google.colab.kernel.accessAllowed && !cache) {\n","                            return;\n","                        }\n","                        element.appendChild(document.createTextNode(''));\n","                        const url = await google.colab.kernel.proxyPort(port, {cache});\n","\n","                        const external_link = document.createElement('div');\n","                        external_link.innerHTML = `\n","                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n","                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n","                                    https://localhost:${port}${path}\n","                                </a>\n","                            </div>\n","                        `;\n","                        element.appendChild(external_link);\n","\n","                        const iframe = document.createElement('iframe');\n","                        iframe.src = new URL(path, url).toString();\n","                        iframe.height = height;\n","                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n","                        iframe.width = width;\n","                        iframe.style.border = 0;\n","                        element.appendChild(iframe);\n","                    })(7861, \"/\", \"100%\", 500, false, window.element)"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":24}]},{"cell_type":"markdown","source":["### ATTBILSTM\n","Dropout 직접 제거 - 여전히 0.85로 됨. 코드상의 문제이거나 런타임에 저장된 변수 문제일 수 있다.\n"],"metadata":{"id":"iIODheyWI4Co"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import DataLoader, TensorDataset\n","from tqdm import tqdm\n","from transformers import ElectraTokenizer, AdamW\n","import gradio as gr\n","\n","# Define Attention Layer\n","class Attention(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(Attention, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.W = nn.Linear(hidden_size, hidden_size)\n","        self.v = nn.Linear(hidden_size, 1, bias=False)\n","\n","    def forward(self, encoder_outputs):\n","        energy = torch.tanh(self.W(encoder_outputs))\n","        attention_scores = self.v(energy).squeeze(2)\n","        attention_weights = torch.softmax(attention_scores, dim=1)\n","        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n","        return context\n","\n","# Define Bi-LSTM with Attention\n","class BiLSTMWithAttention(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_classes):\n","        super(BiLSTMWithAttention, self).__init__()\n","        self.embedding = nn.Embedding(input_size, hidden_size)\n","        self.lstm = nn.LSTM(hidden_size, hidden_size, bidirectional=True)\n","        self.attention = Attention(hidden_size * 2)\n","        self.fc = nn.Linear(hidden_size * 2, num_classes)\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x)\n","        lstm_out, _ = self.lstm(embedded)\n","        context = self.attention(lstm_out)\n","        output = self.fc(context)\n","        return output\n","\n","# Load data\n","data = pd.read_csv(\"/content/drive/MyDrive/intergrated_unbalan5.csv\")\n","\n","# Separate text and label columns\n","texts = data['text'].tolist()\n","labels = data['label'].tolist()\n","\n","# Separate training and validation data\n","train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n","\n","# Load the Electra tokenizer\n","tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-discriminator\", use_fast=False)\n","\n","# Tokenization and padding\n","max_length = 128\n","train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n","val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n","\n","# Create a PyTorch dataset\n","train_dataset = TensorDataset(train_encodings['input_ids'],\n","                              train_encodings['attention_mask'],\n","                              torch.tensor(train_labels))\n","val_dataset = TensorDataset(val_encodings['input_ids'],\n","                            val_encodings['attention_mask'],\n","                            torch.tensor(val_labels))\n","\n","# Create data loader\n","batch_size = 16\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","\n","# Model settings\n","input_size = len(tokenizer)\n","hidden_size = 256\n","num_classes = 2\n","learning_rate = 2e-5\n","\n","model = BiLSTMWithAttention(input_size, hidden_size, num_classes)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","optimizer = AdamW(model.parameters(), lr=learning_rate)\n","\n","# Learning\n","num_epochs = 5\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n","        input_ids, attention_mask, labels = batch\n","        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(input_ids)\n","        loss_fn = nn.CrossEntropyLoss()\n","        loss = loss_fn(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    correct_predictions = 0\n","    total_predictions = 0\n","    with torch.no_grad():\n","        for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n","            input_ids, attention_mask, labels = batch\n","            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","\n","            outputs = model(input_ids)\n","            val_loss += loss_fn(outputs, labels).item()\n","\n","            predicted_labels = outputs.argmax(dim=1)\n","            correct_predictions += (predicted_labels == labels).sum().item()\n","            total_predictions += labels.size(0)\n","\n","    val_accuracy = correct_predictions / total_predictions\n","    # Calculate average loss\n","    avg_train_loss = total_loss / len(train_loader)\n","    print(f\"Epoch {epoch+1}/{num_epochs}: Avg Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n","\n","# Save the trained model\n","model_save_path = \"/content/drive/MyDrive/AttBiLSTM_2K\"\n","torch.save(model.state_dict(), model_save_path)\n","\n","# Define the prediction function\n","def predict(text):\n","    encoding = tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\")\n","    input_ids = encoding['input_ids'].to(device)\n","    with torch.no_grad():\n","        outputs = model(input_ids)\n","        probabilities = torch.softmax(outputs, dim=1).tolist()[0]\n","\n","    return {str(label): prob for label, prob in enumerate(probabilities)}\n","\n","# Define the Gradio interface\n","iface = gr.Interface(fn=predict, inputs=\"text\", outputs=gr.outputs.JSON())\n","iface.launch()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":410},"id":"p0_Y2XU4IceX","executionInfo":{"status":"error","timestamp":1692909906748,"user_tz":-540,"elapsed":4741,"user":{"displayName":"hufs_1jo","userId":"13927265952478603851"}},"outputId":"495264fa-33d4-443a-affa-27fef4fcedff"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stderr","text":["Epoch 4/5 - Training:  23%|██▎       | 32/140 [00:15<00:50,  2.12it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-c09cc136caa3>\u001b[0m in \u001b[0;36m<cell line: 88>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-25-c09cc136caa3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    813\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    814\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["### ATTBILSTM (KoELECTRA의 토크나이저 사용)\n","Dropout 삭제 요청 - 문제 해결\n","\n","가장 성능 좋은 모델 도출"],"metadata":{"id":"ZCdV0cUuTkkg"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import DataLoader, TensorDataset\n","from tqdm import tqdm\n","from transformers import ElectraTokenizer, AdamW\n","import gradio as gr\n","\n","# Define Attention Layer\n","class Attention(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(Attention, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.W = nn.Linear(hidden_size, hidden_size)\n","        self.v = nn.Linear(hidden_size, 1, bias=False)\n","\n","    def forward(self, encoder_outputs):\n","        energy = torch.tanh(self.W(encoder_outputs))\n","        attention_scores = self.v(energy).squeeze(2)\n","        attention_weights = torch.softmax(attention_scores, dim=1)\n","        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n","        return context\n","\n","# Define Bi-LSTM with Attention\n","class BiLSTMWithAttention(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_classes):\n","        super(BiLSTMWithAttention, self).__init__()\n","        self.embedding = nn.Embedding(input_size, hidden_size)\n","        self.lstm = nn.LSTM(hidden_size, hidden_size, bidirectional=True)\n","        self.attention = Attention(hidden_size * 2)\n","        self.fc = nn.Linear(hidden_size * 2, num_classes)\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x)\n","        lstm_out, _ = self.lstm(embedded)\n","        context = self.attention(lstm_out)\n","        output = self.fc(context)\n","        return output\n","\n","# Load data\n","data = pd.read_csv('/content/drive/MyDrive/intergrated_unbalan5.csv')\n","\n","# Separate text and label columns\n","texts = data['text'].tolist()\n","labels = data['label'].tolist()\n","\n","# Separate training and validation data\n","train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n","\n","# Load the Electra tokenizer\n","tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-discriminator\", use_fast=False)\n","\n","# Tokenization and padding\n","max_length = 128\n","train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n","val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n","\n","# Create a PyTorch dataset\n","train_dataset = TensorDataset(train_encodings['input_ids'],\n","                              train_encodings['attention_mask'],\n","                              torch.tensor(train_labels))\n","val_dataset = TensorDataset(val_encodings['input_ids'],\n","                            val_encodings['attention_mask'],\n","                            torch.tensor(val_labels))\n","\n","# Create data loader\n","batch_size = 16\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","\n","# Model settings\n","input_size = len(tokenizer)\n","hidden_size = 256\n","num_classes = 2\n","\n","model = BiLSTMWithAttention(input_size, hidden_size, num_classes)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","optimizer = AdamW(model.parameters())\n","\n","# Learning\n","num_epochs = 3\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n","        input_ids, attention_mask, labels = batch\n","        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(input_ids)\n","        loss_fn = nn.CrossEntropyLoss()\n","        loss = loss_fn(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    correct_predictions = 0\n","    total_predictions = 0\n","    with torch.no_grad():\n","        for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n","            input_ids, attention_mask, labels = batch\n","            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","\n","            outputs = model(input_ids)\n","            val_loss += loss_fn(outputs, labels).item()\n","\n","            predicted_labels = outputs.argmax(dim=1)\n","            correct_predictions += (predicted_labels == labels).sum().item()\n","            total_predictions += labels.size(0)\n","\n","    val_accuracy = correct_predictions / total_predictions\n","    avg_val_loss = val_loss / len(val_loader)\n","    print(f\"Epoch {epoch+1}/{num_epochs}: Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n","\n","# Save the trained model\n","model_save_path = \"/content/drive/MyDrive/AttBiLSTM_2K\"\n","torch.save(model.state_dict(), model_save_path)\n","\n","# Define the prediction function\n","def predict(text):\n","    encoding = tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\")\n","    input_ids = encoding['input_ids'].to(device)\n","    with torch.no_grad():\n","        outputs = model(input_ids)\n","        probabilities = torch.softmax(outputs, dim=1).tolist()[0]\n","\n","    return {str(label): prob for label, prob in enumerate(probabilities)}\n","\n","# Define the Gradio interface\n","iface = gr.Interface(fn=predict, inputs=\"text\", outputs=gr.outputs.JSON())\n","iface.launch()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":862},"id":"SU72U_RGTPZE","executionInfo":{"status":"ok","timestamp":1692910315857,"user_tz":-540,"elapsed":273126,"user":{"displayName":"hufs_1jo","userId":"13927265952478603851"}},"outputId":"345cdf04-c90b-4a77-8f18-ae1ffe3f21b5"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Epoch 1/3 - Training: 100%|██████████| 140/140 [01:53<00:00,  1.23it/s]\n","Epoch 1/3 - Validation: 100%|██████████| 35/35 [00:07<00:00,  4.47it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/3: Val Loss: 0.2774, Val Acc: 0.8982\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/3 - Training: 100%|██████████| 140/140 [01:05<00:00,  2.14it/s]\n","Epoch 2/3 - Validation: 100%|██████████| 35/35 [00:04<00:00,  8.23it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2/3: Val Loss: 0.1241, Val Acc: 0.9554\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/3 - Training: 100%|██████████| 140/140 [01:07<00:00,  2.06it/s]\n","Epoch 3/3 - Validation: 100%|██████████| 35/35 [00:05<00:00,  6.67it/s]\n","<ipython-input-27-ee79fb1ab99f>:135: GradioDeprecationWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n","  iface = gr.Interface(fn=predict, inputs=\"text\", outputs=gr.outputs.JSON())\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3/3: Val Loss: 0.0631, Val Acc: 0.9804\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n","\n","To create a public link, set `share=True` in `launch()`.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["(async (port, path, width, height, cache, element) => {\n","                        if (!google.colab.kernel.accessAllowed && !cache) {\n","                            return;\n","                        }\n","                        element.appendChild(document.createTextNode(''));\n","                        const url = await google.colab.kernel.proxyPort(port, {cache});\n","\n","                        const external_link = document.createElement('div');\n","                        external_link.innerHTML = `\n","                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n","                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n","                                    https://localhost:${port}${path}\n","                                </a>\n","                            </div>\n","                        `;\n","                        element.appendChild(external_link);\n","\n","                        const iframe = document.createElement('iframe');\n","                        iframe.src = new URL(path, url).toString();\n","                        iframe.height = height;\n","                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n","                        iframe.width = width;\n","                        iframe.style.border = 0;\n","                        element.appendChild(iframe);\n","                    })(7862, \"/\", \"100%\", 500, false, window.element)"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import DataLoader, TensorDataset\n","from tqdm import tqdm\n","from transformers import ElectraTokenizer, AdamW\n","import gradient as gr\n","\n","\n","# Load the model\n","model_save_path = \"/content/drive/MyDrive/AttBiLSTM_2K\"\n","loaded_model = BiLSTMWithAttention(input_size, hidden_size, num_classes)\n","loaded_model.load_state_dict(torch.load(model_save_path))\n","loaded_model.to(device)\n","loaded_model.eval()\n","\n","# Define the prediction function for the loaded model\n","def predict_loaded_model(text):\n","    encoding = tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\")\n","    input_ids = encoding['input_ids'].to(device)\n","    with torch.no_grad():\n","        outputs = loaded_model(input_ids)\n","        probabilities = torch.softmax(outputs, dim=1).tolist()[0]\n","\n","    return {str(label): prob for label, prob in enumerate(probabilities)}\n","\n","# Define the Gradio interface for the loaded model\n","loaded_iface = gr.Interface(fn=predict_loaded_model, inputs=\"text\", outputs=gr.outputs.JSON())\n","loaded_iface.launch()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":671},"id":"kiEojxsDZ8JL","executionInfo":{"status":"ok","timestamp":1692911783212,"user_tz":-540,"elapsed":796,"user":{"displayName":"hufs_1jo","userId":"13927265952478603851"}},"outputId":"bb187a58-ca24-4c5a-8514-3ab078248cad"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-35-9b8e153f9263>:18: GradioDeprecationWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n","  loaded_iface = gr.Interface(fn=predict_loaded_model, inputs=\"text\", outputs=gr.outputs.JSON())\n"]},{"output_type":"stream","name":"stdout","text":["Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n","\n","To create a public link, set `share=True` in `launch()`.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["(async (port, path, width, height, cache, element) => {\n","                        if (!google.colab.kernel.accessAllowed && !cache) {\n","                            return;\n","                        }\n","                        element.appendChild(document.createTextNode(''));\n","                        const url = await google.colab.kernel.proxyPort(port, {cache});\n","\n","                        const external_link = document.createElement('div');\n","                        external_link.innerHTML = `\n","                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n","                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n","                                    https://localhost:${port}${path}\n","                                </a>\n","                            </div>\n","                        `;\n","                        element.appendChild(external_link);\n","\n","                        const iframe = document.createElement('iframe');\n","                        iframe.src = new URL(path, url).toString();\n","                        iframe.height = height;\n","                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n","                        iframe.width = width;\n","                        iframe.style.border = 0;\n","                        element.appendChild(iframe);\n","                    })(7863, \"/\", \"100%\", 500, false, window.element)"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":35}]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"collapsed_sections":["R8g6qlnZ2bqJ","JqMXNQeD2io1","BaGl83Hr8rof","VCdskC3T_VUf","tSsCWbylIZj2","EVtEd6L5PiwJ","iIODheyWI4Co"],"gpuType":"T4","mount_file_id":"1v4UjNhGRKEGhD77MoX7Azqa4cMq1ypJJ","authorship_tag":"ABX9TyO7POMh3RiFPkwjkMVRC6DK"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}