{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### 모듈화 완료"],"metadata":{"id":"H4w5isCudTyv"}},{"cell_type":"markdown","source":["text_classification_module.py"],"metadata":{"id":"pqMekoXzddhf"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from transformers import ElectraTokenizer\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","class Attention(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(Attention, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.W = nn.Linear(hidden_size, hidden_size)\n","        self.v = nn.Linear(hidden_size, 1, bias=False)\n","\n","    def forward(self, encoder_outputs):\n","        energy = torch.tanh(self.W(encoder_outputs))\n","        attention_scores = self.v(energy).squeeze(2)\n","        attention_weights = torch.softmax(attention_scores, dim=1)\n","        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n","        return context\n","\n","class BiLSTMWithAttention(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_classes):\n","        super(BiLSTMWithAttention, self).__init__()\n","        self.embedding = nn.Embedding(input_size, hidden_size)\n","        self.lstm = nn.LSTM(hidden_size, hidden_size, bidirectional=True)\n","        self.attention = Attention(hidden_size * 2)\n","        self.fc = nn.Linear(hidden_size * 2, num_classes)\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x)\n","        lstm_out, _ = self.lstm(embedded)\n","        context = self.attention(lstm_out)\n","        output = self.fc(context)\n","        return output\n","\n","class TextClassifier:\n","    def __init__(self, model_save_path):\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-discriminator\", use_fast=False)\n","        self.model = None\n","        self.load_model(model_save_path)\n","\n","    def load_model(self, model_save_path):\n","        self.model = BiLSTMWithAttention(input_size=len(self.tokenizer), hidden_size=256, num_classes=2)\n","        self.model.load_state_dict(torch.load(model_save_path, map_location=self.device))\n","        self.model.to(self.device)\n","        self.model.eval()\n","\n","    def classify_text(self, text):\n","        encoding = self.tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\")\n","        input_ids = encoding['input_ids'].to(self.device)\n","        with torch.no_grad():\n","            outputs = self.model(input_ids)\n","            probabilities = torch.softmax(outputs, dim=1).tolist()[0]\n","\n","        label_names = ['Label 0', 'Label 1']\n","        label_probabilities = {label: prob for label, prob in zip(label_names, probabilities)}\n","        return label_probabilities\n"],"metadata":{"id":"9oujXY-_EfEC","executionInfo":{"status":"ok","timestamp":1692942052524,"user_tz":-540,"elapsed":397,"user":{"displayName":"hufs_1jo","userId":"13927265952478603851"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["모듈을 불러와 실행할 파일.py"],"metadata":{"id":"LlgjDPYTdmHf"}},{"cell_type":"code","source":["# text_classification_module.py를 불러온다.\n","from text_classification_module import TextClassifier\n","\n","def main():\n","    model_save_path = \"/content/drive/MyDrive/AttBiLSTM_2K\"\n","    classifier = TextClassifier(model_save_path)\n","\n","    example_text = input(\"대화를 입력하세요. (최소 50자/20어절/2문장 이상)\")\n","    result = classifier.classify_text(example_text)\n","\n","    for label, prob in result.items():\n","        print(f\"{int(prob * 100)}% 확률로 {label} 라벨로 분류됩니다.\")\n","\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"q02XXqEyLKaD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 모듈화 이전 코드"],"metadata":{"id":"sGgH2UinLFTf"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# 데이터셋 및 경로 설정\n","data = pd.read_csv('/content/drive/MyDrive/intergrated_unbalan5.csv') #데이터셋\n","model_save_path = \"/content/drive/MyDrive/AttBiLSTM_2K\" # 모델\n","\n","!pip install transformers\n","!pip install accelerate>=0.20.1\n","!pip install transformers[torch]\n","!pip install accelerate transformers[torch]\n","\n","import torch\n","import torch.nn as nn\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import DataLoader, TensorDataset\n","from tqdm import tqdm\n","from transformers import ElectraTokenizer, AdamW\n","\n","\n","# Define Attention Layer\n","class Attention(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(Attention, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.W = nn.Linear(hidden_size, hidden_size)\n","        self.v = nn.Linear(hidden_size, 1, bias=False)\n","\n","    def forward(self, encoder_outputs):\n","        energy = torch.tanh(self.W(encoder_outputs))\n","        attention_scores = self.v(energy).squeeze(2)\n","        attention_weights = torch.softmax(attention_scores, dim=1)\n","        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n","        return context\n","\n","# Define Bi-LSTM with Attention\n","class BiLSTMWithAttention(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_classes):\n","        super(BiLSTMWithAttention, self).__init__()\n","        self.embedding = nn.Embedding(input_size, hidden_size)\n","        self.lstm = nn.LSTM(hidden_size, hidden_size, bidirectional=True)\n","        self.attention = Attention(hidden_size * 2)\n","        self.fc = nn.Linear(hidden_size * 2, num_classes)\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x)\n","        lstm_out, _ = self.lstm(embedded)\n","        context = self.attention(lstm_out)\n","        output = self.fc(context)\n","        return output\n","\n","# Separate text and label columns\n","texts = data['text'].tolist()\n","labels = data['label'].tolist()\n","\n","# Separate training and validation data\n","train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n","\n","# Load the Electra tokenizer\n","tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-discriminator\", use_fast=False)\n","\n","# Tokenization and padding\n","max_length = 128\n","train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n","val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n","\n","# Create a PyTorch dataset\n","train_dataset = TensorDataset(train_encodings['input_ids'],\n","                              train_encodings['attention_mask'],\n","                              torch.tensor(train_labels))\n","val_dataset = TensorDataset(val_encodings['input_ids'],\n","                            val_encodings['attention_mask'],\n","                            torch.tensor(val_labels))\n","\n","# Create data loader\n","batch_size = 16\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","\n","# Model settings\n","input_size = len(tokenizer)\n","hidden_size = 256\n","num_classes = 2\n","\n","model = BiLSTMWithAttention(input_size, hidden_size, num_classes)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","optimizer = AdamW(model.parameters())\n","\n","# Load the model\n","loaded_model = BiLSTMWithAttention(input_size, hidden_size, num_classes)\n","loaded_model.load_state_dict(torch.load(model_save_path))\n","loaded_model.to(device)\n","loaded_model.eval()\n","\n","# Define the prediction function for the loaded model\n","def predict_loaded_model(text):\n","    encoding = tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\")\n","    input_ids = encoding['input_ids'].to(device)\n","    with torch.no_grad():\n","        outputs = loaded_model(input_ids)\n","        probabilities = torch.softmax(outputs, dim=1).tolist()[0]\n","\n","    return {str(label): prob for label, prob in enumerate(probabilities)}\n","# gradio 인터페이스로 구현\n","'''\n","!pip install gradio\n","import gradio as gr\n","# Define the Gradio interface for the loaded model\n","loaded_iface = gr.Interface(fn=predict_loaded_model, inputs=\"text\", outputs=gr.outputs.JSON())\n","loaded_iface.launch()\n","'''\n","# 입력\n","def classify_text_without_gr(text):\n","    # Use the loaded_model and tokenizer\n","    encoding = tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\")\n","    input_ids = encoding['input_ids'].to(device)\n","\n","    with torch.no_grad():\n","        outputs = loaded_model(input_ids)\n","        probabilities = torch.softmax(outputs, dim=1).tolist()[0]\n","\n","    label_names = ['Label 0', 'Label 1']  # Modify label names accordingly\n","\n","    label_probabilities = {label: prob for label, prob in zip(label_names, probabilities)}\n","    return label_probabilities"],"metadata":{"id":"QOZH883Pa6iU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 예시 텍스트 입력 , 함수 호출 및 결과 출력\n","# 학습 데이터 특성상 최소 3문장 이상, 40자 이상의 문답 형식으로 입력하실 것을 권합니다.\n","# 너무 짧거나 맥락이 없는 내용의 나열은 높은 오탐률이 나타날 수 있습니다.\n","example_text = input (\"대화 입력:\")\n","result = classify_text_without_gr(example_text)\n","if result['Label 1'] >= 0.001:\n","    print(f\"{int(result['Label 1'] * 100)}% 확률로 피싱입니다.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5EVhRsdPPkU6","executionInfo":{"status":"ok","timestamp":1692946192252,"user_tz":-540,"elapsed":3449,"user":{"displayName":"hufs_1jo","userId":"13927265952478603851"}},"outputId":"55213bf0-f92f-413c-ed88-51757051c7c7"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["대화 입력:ㅇㅇ\n","0% 확률로 피싱입니다.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"BhYq2GrHbwTn"},"execution_count":null,"outputs":[]}]}